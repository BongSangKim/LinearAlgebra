{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기계학습(machine learning)의 기본개념\n",
    "\n",
    "## 데이터의 종류\n",
    "\n",
    "기계학습은 데이터를 예측하는 모델의 정확한 변수값을 모르는 상태에서 이미 주어진 데이터를 통해 최적의 변수를 찾아내는 과정입니다.\n",
    "\n",
    "이를 위해 훈련 데이터(training data)와 시험 데이터(test data)가 필요합니다.\n",
    "\n",
    "> 훈련 데이터는 기본적으로 변수값을 최적화시키기 위해 사용되는 데이터 입니다.\n",
    ">\n",
    "> 훈련 데이터가 부정확하다면, 결과도 부정확합니다. (**Garbage in, garbage out**)\n",
    ">\n",
    "> 따라서 기계학습을 성공적으로 수행하기 위한 첫번째 단계는 좋은 훈련 데이터를 준비하는 것입니다.\n",
    "\n",
    "훈련 데이터를 이용하여 변수값을 찾았다면 이것이 정확한지 테스트 하기 위해 사용하는 데이터가 시험 데이터입니다.\n",
    "\n",
    "일반적으로 시험 데이터는 훈련 데이터의 크기보다 작습니다.\n",
    "\n",
    "보통 데이터는 훈련용과 시험용이 구분되어 있지 않으므로 무작위로 데이터를 구분하여 사용합니다.\n",
    "\n",
    "이를 미니배치(mini-batch)라 합니다.\n",
    "\n",
    "> 만약 시험 데이터가 지나치게 적거나 편향되어 있다면 어떤 문제가 생길 수 있나요?\n",
    ">\n",
    "> 모델이 훈련 데이터에 지나치게 최적화 된 상태를 오버피팅(overfitting)이라고 합니다.\n",
    ">\n",
    "> 오버피팅은 잘못된 모델링으로 인해 일어날 수도 있지만, 잘못된 데이터로 인해 일어날 수도 있습니다.\n",
    ">\n",
    "> 예를 들어 사진을 '개' 또는 '고양이'를 분류하는 모델을 학습시킨다고 합시다.\n",
    ">\n",
    "> '개'의 사진만을 가지고 학습 시킨 모델은 '개'는 정확히 맞출 수 있으나 '고양이'는 정확히 맞출 수가 없습니다.\n",
    ">\n",
    "> (오히려 '고양이'를 '개'라고 잘못 분류하는 경우가 더 많을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수(loss function)\n",
    "\n",
    "모델링을 최적화시키기 위해서는 정확도가 향상되는 방향으로 조금씩 변수값을 조정해야 합니다.\n",
    "\n",
    "다시 말하면 정확도가 떨어짐으로서 발생하는 손실(loss)이 최소화되어야 합니다.\n",
    "\n",
    "손실 함수란 기계 학습의 정확도를 가리키는 지표입니다.\n",
    "\n",
    "### 1. 평균 제곱 오차(mean squre error)\n",
    "\n",
    "평균제곱오차는 가장 많이 쓰이는 손실함수의 하나입니다.\n",
    "\n",
    "모델 예측값 $y_k$라 하고 실제값을 $t_k$라 하면 평균 제곱 오차 $E$는 다음과 같습니다.\n",
    "\n",
    "$$E = \\frac{1}{2}\\sum_{k=1}^N(y_k-t_k)^2$$\n",
    "\n",
    "여기서 $N$은 데이터의 크기입니다. \n",
    "\n",
    "편차의 제곱을 하는 이유는 참값($t_k$)보다 실제값($y_k$)이 작거나 큰 경우를 동일하게 보기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msq(y,t):\n",
    "    return .5 * np.sum((y-t) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 교차 엔트로피 오차\n",
    "\n",
    "교차 엔트로피 오차는(cross-entropy error)는 평균 제곱 오차와는 약간 성격이 다릅니다. 모델 예측값을 $y_k$ 참값을 $t_k$라 합시다. 교차 엔트로피 오차는 다음과 같이 정의됩니다.\n",
    "\n",
    "$$E = -\\sum_k t_k\\log y_k$$\n",
    "\n",
    "만약 예측하고자 하는 값이 참(1) 또는 거짓(0)이라면 모든 $t_k$는 $0$ 또는 $1$로 기록되어 있습니다. 그러나 예측값 $y_k$는 $0$에서 $1$사이의 실수값으로 기록될 것입니다. 먄약 $t_k$가 $0$이라면 $t_k\\log y_k$는 항상 $0$입니다. 만약 $t_k$가 $1$이라면 $t_k\\log y_k$는 $y_k$이 클수록 더 작은 음의 실수를 가리킵니다. 오차값 $E$에 음의 부호가 붙어있으므로, $E$는 항상 양수입니다. 또한 $t_k$가 $1$인 $y_k$가 $1$에 가까울수록 $E$가 작아짐을 알 수 있습니다. 다시 말해, 참값이 $1$인 데이터를 잘 예측할 수록 $E$가 작아집니다.\n",
    "\n",
    "실제로, 예측값 $y_k$가 $0$이 되면 $\\log$값이 마이너스 무한대를 반환하므로 계산에 오류가 발생합니다. 따라서 실제 구현에서는 $y_k$ 값에 아주 작은 $\\delta$를 더하여 $0$이 되지 않게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 교차 엔트로피에 사용된 레이블 $y_k$, $t_k$는 하나의 데이터에 대한 출력 뉴런의 값과 정답값입니다. 예를 들어 MNIST 데이터셋의 경우, 하나의 데이터마다 10개의 레이블(0,1,2,...,9)이 있습니다. 기계학습을 위해서는 아주 많은 수의 데이터를 학습시켜야 하고, 이것에 대한 교차 엔트로피를 구해야 합니다. 즉, $N$개의 데이터가 있다고 하면 각 $n$번째 데이터마다 교차엔트로피를 구해서 평균을 구합니다.\n",
    "$$E = -\\frac{1}{N}\\sum_n\\sum_k t_{nk}\\log y_{nk}$$\n",
    "그런데 이렇게 하면 계산에 많은 시간이 소요됩니다. MNIST 데이터만 해도 6만개의 데이터가 있습니다. 따라서 모든 데이터를 사용하지 않고, 랜덤으로 몇개의 샘플만 추출해서 계산하는 것이 효율적입니다. 이를 **미니배치(mini-batch)**라 합니다. `np.random.choice`함수를 이용하면 무작위로 원하는 갯수의 샘플을 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_minibatch(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기울기벡터(gradient)\n",
    "\n",
    "위와 같은 손실함수의 값을 줄이기 위해서는 현재 가중치와 편향을 손실함수가 작아지는 쪽으로 변화시켜야가야 합니다. 이를 위해 각 기울기벡터(gradient)를 구해야 합니다. 함수 $f(x,y)$에 대한 기울기 벡터는 다음과 같습니다.\n",
    "$$\\nabla f(x_0,y_0) = (f_x(x_0,y_0),f_y(x_0,y_0))$$\n",
    "이를 함수로 구현하면 아래와 같습니다. 실제로 편미분을 구현하기는 복잡하므로, 여기서는 수치적 편미분을 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_1d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_2d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for row in range(x.shape[0]):\n",
    "        for col in range(x.shape[1]):\n",
    "            tmp_val = x[row, col]\n",
    "            x[row, col] = tmp_val + h\n",
    "            fxh1 = f(x)\n",
    "        \n",
    "            x[row, col] = tmp_val - h\n",
    "            fxh2 = f(x)\n",
    "        \n",
    "            grad[row, col] = (fxh1 - fxh2) / (2 * h)\n",
    "            x[row, col] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법(Gradient descent)\n",
    "경사하강법이란 특점 점에서 기울기벡터 방향으로 조금씩 이동하면서 함수의 극소값을 도달하는 방법입니다. 다시말해 $x^{(0)} = (x_0^{(0)},x_1^{(0)})$에서 시작하였을 때, 음의 기울기 벡터 $-\\nabla f(x_0,x_1)$의 방향으로 $\\nu$만큼 이동하여 극소값에 근접한 새로운 점 $x^{(1)} = (x_0^{(1)}, x_1^{(1)}$을 찾는 방법입니다. 이를 수식으로 표현하면 아래와 같습니다.\n",
    "$$x_0^{(1)} = x_0^{(0)} - \\nu\\frac{\\partial f}{\\partial x_0}$$\n",
    "$$x_1^{(1)} = x_1^{(0)} - \\nu\\frac{\\partial f}{\\partial x_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 신경망 모델\n",
    "경사하강법을 이용하여 기계 학습이 구현된 간단한 신경망 모델을 만들어 보겠습니다. 여기서는 **클래스(class)**라는 구조를 사용합니다. 클래스란 딕셔너리 자료형과 비슷하게 변수가 구현될 뿐만 아니라 메서드 함수도 구현되는 자료구조를 말합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    m = np.max(x)\n",
    "    ex = np.exp(x - m)\n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_minibatch(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 `simpleNet`이라는 클래스는 앞서 정의한 `network`를 클래스로 구현한 것입니다. `__init__`은 클래스객체를 초기화하는 함수로서 모든 클래스가 기본으로 구현되어야 합니다. 여기서는 랜덤한 2행 3열의 행렬로 초기화를 시킵니다. 예를 들어 `net`으로 클래스를 생성해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.11724359 -0.69089631  0.43287905]\n",
      " [-0.14126068  0.78843414 -0.50883815]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력 뉴런 값을 임의로 집어넣고 순전파(forward propagation)를 진행해 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54321154  0.29505294 -0.1982269 ]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([.6, .9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 실제값에 대한 참값 레이블이 $[0, 1, 0]$ 이었다고 합니다. (즉, 2번째 뉴런이 가장 크게 활성화가 되어야 정확한 결과입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35401535236446646"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 1, 0])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 손실함수의 값을 더 줄이기 위해 경사하강법을 손실함수 $f$에 적용해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08862636 -0.13085045  0.04222409]\n",
      " [ 0.13293954 -0.19627567  0.06333613]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 기울기벡터 방향으로 가중치를 조금 바꾼 뒤 다시 손실함수를 계산해 봅시다. 위에서 계산한 값보다 조금 줄어드는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3521917591376414"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nu = 1e-2\n",
    "net.W -= nu * dW\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2층 신경망 클래스 구현\n",
    "기계학습은 위 과정을 반복하는 것입니다.\n",
    "1. 레이어의 수, 각 레이어마다 뉴런의 갯수, 가중치와 편향을 고려한 신경망 모델 제작\n",
    "2. 미니배치, 손실함수 설정\n",
    "3. 기울기벡터 구하기\n",
    "4. 매개변수(가중치, 편향 값) 갱신\n",
    "5. 1-4 반복\n",
    "\n",
    "위 방법을 적용하여 2층 신경망 클래스를 구현하고 MNIST 데이터셋으로 검증해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def softmax(x):\n",
    "    m = np.max(x)\n",
    "    ex = np.exp(x - m)\n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "def _numerical_gradient_1d(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient_2d(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_1d(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_1d(f, x)\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.10293333333333334, 0.0979\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(train_loss_list))\n",
    "y = train_loss_list\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
